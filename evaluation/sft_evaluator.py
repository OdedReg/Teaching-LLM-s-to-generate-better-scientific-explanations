import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from utils import clean_text
from evaluation.utils import process_tagging_data, answer

class SFTEvalDataCreator:
    """
    A class for creating a dataset to evaluate and compare responses from a base model and an SFT model.
    """
    def __init__(self, test_df_path, base_model, base_tokenizer, sft_model, sft_tokenizer, device, new_model_name):
        """
        Initializes the SFTEvalDataCreator with necessary models, tokenizers, and data.

        Parameters:
            test_df_path (str): Path to the CSV file with test questions.
            base_model: The base language model for generating responses.
            base_tokenizer: The tokenizer for the base model.
            sft_model: The Supervised Fine-Tuned model for generating responses.
            sft_tokenizer: The tokenizer for the SFT model.
            device (str): The device to use for model inference.
            new_model_name (str): The name of the new model to compare.
        """
        self.test_df = pd.read_csv(test_df_path)
        self.base_model = base_model
        self.base_tokenizer = base_tokenizer
        self.sft_model = sft_model
        self.sft_tokenizer = sft_tokenizer
        self.device = device
        self.new_model_name = new_model_name
        
    def CreateDB(self, save_path):
        """
        Generates a dataset comparing responses from the base model and the SFT model,
        then saves the results to a CSV file.

        Parameters:
            save_path (str): Path to save the resulting CSV file.
        """
        results = []
        for i in range(len(self.test_df)):
            question, base_answer, sft_answer = self._get_responses(i)
            base_answer = clean_text(base_answer)
            sft_answer = clean_text(sft_answer)
            
            results.append(self._prepare_result(question, base_answer, sft_answer))
            
            if i % 5 == 0:
                self._save_results(results, save_path)
        
        self._save_results(results, save_path)
    
    def _get_responses(self, index):
        """
        Retrieves answers from both the base model and the SFT model for a given question.

        Parameters:
            index (int): Index of the question in the test DataFrame.

        Returns:
            tuple: The question, base model answer, and SFT model answer.
        """
        question = self.test_df.iloc[index]['Question']
        base_answer = answer(self.base_model, self.base_tokenizer, question, self.device)[1:]
        sft_answer = answer(self.sft_model, self.sft_tokenizer, question, self.device)
        return question, base_answer, sft_answer
    
    def _prepare_result(self, question, base_answer, sft_answer):
        """
        Prepares a result entry for the dataset by shuffling and selecting answers.

        Parameters:
            question (str): The question text.
            base_answer (str): The answer generated by the base model.
            sft_answer (str): The answer generated by the SFT model.

        Returns:
            dict: A dictionary containing the question, options, and sources.
        """
        answers = [(base_answer, 'Llama_model'), (sft_answer, self.new_model_name)]
        random.shuffle(answers)
        option1, source1 = answers[0]
        option2, source2 = answers[1]
        return {'Question': question, 'option1': option1, 'option2': option2, 'Source1': source1, 'Source2': source2}
    
    def _save_results(self, results, save_path):
        """
        Saves the accumulated results to a CSV file.

        Parameters:
            results (list): List of dictionaries containing the results.
            save_path (str): Path to save the results CSV file.
        """
        results_df = pd.DataFrame(results)
        results_df.to_csv(save_path, index=False)


class SFTEvaluator():
    """
    A class for evaluating and visualizing the performance of different models based on the test data.

    Attributes:
        test_df_path (str): Path to the CSV file containing the test data.
        taggers_path_list (list of str): List of paths to CSV files with tagging results.
    """
    def __init__(self, test_df_path, taggers_path_list):
        """
        Initializes the SFTEvaluator with test data and tagging results.

        Parameters:
            test_df_path (str): Path to the CSV file with test data.
            taggers_path_list (list of str): List of paths to CSV files with tagging results.
        """
        # Load the test DataFrame
        self.test_df = pd.read_csv(test_df_path)
        self.test_df = process_tagging_data(taggers_path_list, self.test_df)

    def analyze_win_rate(self, model_order, model_labels):
        """
        Analyzes the win rate of models based on their performance and generates a bar plot.

        Parameters:
            model_order (list of str): The order of models to be used for categorical conversion.
            model_labels (dict): Dictionary mapping model names to their labels for plotting.
        """
        df = self.test_df.copy()

        # Define the order of categories for Preference Strength
        preference_order = ['Unsure', 'Slightly Better', 'Better', 'Significantly Better']

        # Convert 'Preference Strength' to a categorical type with the specified order
        df['Preference Strength'] = pd.Categorical(df['Preference Strength'], categories=preference_order, ordered=True)

        # Convert 'winner_model' to a categorical type with the specified order
        df['winner_model'] = pd.Categorical(df['winner_model'], categories=model_order, ordered=True)

        # Calculate win rates
        win_rate = df['winner_model'].value_counts(normalize=True) * 100

        # Print win rates with updated labels
        print("Win Rate of Best Model:")
        for model, rate in win_rate.items():
            print(f"{model_labels.get(model, model)}: {rate:.2f}%")

        # Calculate counts and percentages within each 'winner_model' group
        counts = df.groupby(['winner_model', 'Preference Strength']).size().unstack(fill_value=0)
        percentages = counts.div(counts.sum(axis=1), axis=0) * 100

        # Plot the percentage bars
        ax = percentages.T.plot(kind='bar', stacked=False, figsize=(10, 6))

        # Update legend labels
        handles, labels = ax.get_legend_handles_labels()
        labels = [model_labels[label] for label in labels]
        ax.legend(handles, labels, title='Winner Model')

        plt.title('Preference Strength Distribution by Model')
        plt.ylabel('Percentage')
        plt.xlabel('Preference Strength')
        plt.xticks(rotation=45)
        plt.show()