# Teaching LLMs to Generate Better Scientific Explanations

**Oded Regev, Tal Hayun, Lucila Silbermann, Shani Landsberger**  
**Advisor: Dr. Nir Grinberg**  
**August 2024**

## Project Objective

This research project aims to fine-tune an existing large language model (LLM) to enhance its ability to generate high-quality scientific explanations. While LLMs are proficient at generating human-like text, they often fall short in producing clear, coherent, and well-structured scientific content. This project focuses on improving the structural quality and scientific writing level of generated explanations, prioritizing these aspects over factual accuracy.

The ultimate goal is to refine the LLM to produce explanations that are not only comprehensible but also adhere to the conventions of scientific discourse, making them suitable for academic and professional use. The project leverages state-of-the-art methodologies, including Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), to systematically improve the model's performance.

## Methodology

### Quality Metrics Determination
Determining quality metrics for evaluating scientific explanations is inherently challenging due to the subjective and context-dependent nature of scientific discourse. To address this, we conducted an extensive review of literature in science education and communication, focusing on the structure, coherence, and clarity of explanations rather than factual correctness.

### Supervised Fine-Tuning (SFT) Phase
In the SFT phase, we fine-tuned the LLM on a dataset composed of:
1. **Q/A from Scientific Forums:** We collected question-and-answer pairs from platforms like Stack Exchange, assuming that top-voted answers represent high-quality responses.
2. **Generated Answers using ChatGPT 3.5 Turbo:** To supplement the dataset, we used ChatGPT 3.5 Turbo to generate answers that aligned with our predefined scientific quality metrics, using few-shot Chain of Thought (CoT) prompting.

### Reinforcement Learning (RL) Phase
In the RL phase, we trained a reward model to evaluate the quality of the model's outputs. This phase included:
1. **Rejection Sampling Method:** Generating multiple answers for each question and selecting the best one for further training.
2. **Proximal Policy Optimization (PPO):** Fine-tuning the model using PPO to balance exploration and exploitation in generating high-quality explanations.

## Evaluation

### Supervised Fine-Tuning (SFT) Evaluation
- **Dataset Comparison:** Comparing answers from the base model and SFT model.
- **Human Annotation:** Four annotators reviewed and tagged the preferred answers.
- **Win Rate & Agreement Rate:** Calculating the win rate by majority vote and examining the agreement among annotators.
- **Preference Rating:** Annotators rated their preferences on a four-point scale.

### Reward Model (RM) Evaluation
- **RM Scoring:** Scoring answers generated by the base and SFT models using the reward model.
- **Selection by Max Score:** Choosing the highest-scoring answers.
- **Agreement Rate with Human Annotators:** Comparing RM selections with human annotators' preferences.

### Reinforcement Learning (RLHF) Evaluation
- **RL vs. Base Model:** Human annotators compared answers from the RL and base models.
- **RL vs. GPT:** Comparing the RL model with GPT to ensure it exceeded the GPT model's performance.
- **Reward Model Evaluation:** Statistical analysis of the scores for answers generated by the base, SFT, RLHF, and GPT models.

## Tests

### Model Selection
We chose the Llama-2-7b-chat-hf model for its balance of power and accessibility, being the largest model that could be trained on a single GPU.

### Training Methodology
To stay within GPU limits, we implemented:
1. **Efficient Tokenization:** Managing input size to stay within memory constraints.
2. **LoRA PEFT:** Fine-tuning with low-rank updates to reduce computational overhead.
3. **4-Bit Quantization:** Reducing model size and computational demands.

### Training Phases
1. **Initial SFT Training:** Using Q/A from StackExchange, Reddit, and Wikipedia, which led to poor results, prompting a revised approach.
2. **Revised SFT Training:** Generating answers with GPT-3.5 Turbo, leading to significant improvements.
3. **Reward Model Training:** Annotating 1,800 Q/A pairs to train the RM, achieving a 90% alignment rate with human annotations.
4. **Reinforcement Learning with Human Feedback (RLHF):** Using rejection sampling and PPO, although PPO led to degraded quality and was excluded from the final results.

## Results

- **SFT Phase:** Revised SFT achieved an 80% win rate and a 90% agreement rate among annotators.
- **Reward Model:** RM training showed a 90% alignment rate with human annotations.
- **RLHF Phase:** Rejection sampling led to an 87% win rate, but the PPO stage degraded answer quality.

## Conclusion

This project demonstrated the effectiveness of fine-tuning and RLHF in improving LLMs' ability to generate high-quality scientific explanations. However, the challenges encountered during the PPO stage highlight the need for further research in fine-tuning large language models.
